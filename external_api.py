import requests
import pandas as pd
import os
from datetime import datetime
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

import requests
from bs4 import BeautifulSoup
import os

# í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ëŒ€ì…
API_KEY = os.getenv("LAW_API_KEY") # ì˜ˆì‹œ: g4c@korea.kr â†’ "g4c"

def get_law_id_and_mst(law_name: str) -> tuple[str, str]:
    """ë²•ë ¹ëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ (ë²•ë ¹ID, MSTë²ˆí˜¸) ì¶”ì¶œ"""
    url = "http://www.law.go.kr/DRF/lawSearch.do"
    params = {
        "OC": API_KEY,
        "target": "law",
        "type": "XML",
        "query": law_name,
        "display": 1
    }

    res = requests.get(url, params=params)
    res.raise_for_status()
    soup = BeautifulSoup(res.content, "lxml-xml")
    law_id = soup.find("ë²•ë ¹ID")
    mst = soup.find("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸")  # lawService.do?MST= ì— ì‚¬ìš©
    if not law_id or not mst:
        raise ValueError("âŒ í•´ë‹¹ ë²•ë ¹ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return law_id.text.strip(), mst.text.strip()


def get_law_full_text(mst: str) -> str:
    """MSTë¡œ ì „ì²´ ë³¸ë¬¸ì„ XMLë¡œ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    url = "http://www.law.go.kr/DRF/lawService.do"
    params = {
        "OC": API_KEY,
        "target": "law",
        "type": "XML",
        "MST": mst
    }

    res = requests.get(url, params=params)
    res.raise_for_status()
    soup = BeautifulSoup(res.content, "xml")
    return soup.get_text(separator="\n").strip()


def save_law_text_by_name(law_name: str, save_path: str):
    """ë²•ë ¹ëª…ìœ¼ë¡œ ì „ì²´ ì „ë¬¸ ì €ì¥"""
    try:
        print(f"ğŸ” '{law_name}' ê²€ìƒ‰ ì¤‘...")
        law_id, mst = get_law_id_and_mst(law_name)
        print(f"âœ… ë²•ë ¹ID: {law_id}, MST: {mst}")

        print("ğŸ“¥ ë³¸ë¬¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        full_text = get_law_full_text(mst)

        with open(save_path, "w", encoding="utf-8") as f:
            f.write(full_text)
        print(f"ğŸ“„ ì €ì¥ ì™„ë£Œ: {save_path}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

CORP_NAME_ALIAS = {
    "ì‚¼ì„±ì „ì": "ì‚¼ì„±ì „ìãˆœ",
    "í˜„ëŒ€ì°¨": "í˜„ëŒ€ìë™ì°¨",
    "ì¹´ì¹´ì˜¤": "ãˆœì¹´ì¹´ì˜¤",
    "ë„¤ì´ë²„": "ë„¤ì´ë²„(ì£¼)",
    # í•„ìš”ì‹œ í™•ì¥
}

def fetch_law_detail(law_name, oc_id="g4c"):
    base_url = "http://www.law.go.kr/DRF/lawSearch.do"
    params = {
        "OC": "rhxodud9709",
        "target": "law",
        "type": "HTML",
        "query": law_name,
        "search": 1
    }

    try:
        search_res = requests.get(base_url, params=params, timeout=5)
        search_res.raise_for_status()
        soup = BeautifulSoup(search_res.text, "html.parser")
        first_result = soup.select_one("table tbody tr td a")
        if not first_result:
            return {"summary": "âŒ í•´ë‹¹ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        law_link = "http://www.law.go.kr" + first_result["href"]
        law_detail = requests.get(law_link, timeout=5)
        law_detail.raise_for_status()
        soup_detail = BeautifulSoup(law_detail.text, "html.parser")
        content = soup_detail.get_text()
        summary = content.strip()[:1000] + "..."  # ìš”ì•½ ì²˜ë¦¬
        return {"summary": summary}
    except Exception as e:
        return {"summary": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"}


def fetch_dart_summary(corp_name):
    df = pd.read_csv("corp_list.csv")
    df["corp_code"] = df["corp_code"].astype(str).str.zfill(8)  # â† í•µì‹¬!
    df["corp_name_clean"] = df["corp_name"].astype(str).str.replace("ãˆœ", "").str.strip().str.lower()
    search_name = corp_name.strip().lower()

    # 1ï¸âƒ£ ì™„ì „ì¼ì¹˜ ìš°ì„ 
    exact_match = df[df["corp_name_clean"] == search_name]
    if not exact_match.empty:
        corp_code = exact_match.iloc[0]["corp_code"]
    else:
        # 2ï¸âƒ£ í¬í•¨ ê²€ìƒ‰ fallback
        partial_match = df[df["corp_name_clean"].str.contains(search_name)]
        if partial_match.empty:
            return {"summary": f"âŒ '{corp_name}'ì— ëŒ€í•œ ê¸°ì—… ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        corp_code = partial_match.iloc[0]["corp_code"]

    api_key = os.getenv("DART_API_KEY")
    today = datetime.today().strftime("%Y%m%d")

    params = {
        "crtfc_key": api_key,
        "corp_code": corp_code,
        "bgn_de": "20240101",
        "end_de": today,
        "page_count": 5
    }

    print("[DEBUG] ìš”ì²­ íŒŒë¼ë¯¸í„°:", params)

    try:
        res = requests.get("https://opendart.fss.or.kr/api/list.json", params=params)
        print("[DEBUG] ì‘ë‹µ ìƒíƒœì½”ë“œ:", res.status_code)
        print("[DEBUG] ì‘ë‹µ ë³¸ë¬¸ (ì• 300ì):", res.text[:300])
        data = res.json()
    except Exception as e:
        return {"summary": f"âŒ ìš”ì²­ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}"}

    report_list = data.get("list", [])
    if not report_list:
        return {"summary": f"{corp_name}ì— ëŒ€í•œ ìµœê·¼ ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤."}

    summary = "\n".join([f"- {r['report_nm']} ({r['rcept_dt']})" for r in report_list])
    return {"summary": f"{corp_name}ì˜ ìµœê·¼ ê³µì‹œ ëª©ë¡:\n{summary}"}

# ë²• ì „ë¬¸ ë‹¤ìš´ë¡œë“œ
if __name__ == "__main__":
    # í‡´ì§ì—°ê¸ˆ ê´€ë ¨ ë²•ë ¹ ë¦¬ìŠ¤íŠ¸
    retirement_laws = [
        "ê·¼ë¡œìí‡´ì§ê¸‰ì—¬ë³´ì¥ë²•",
        "êµ­ë¯¼ì—°ê¸ˆë²•",
    ]

    for law_name in retirement_laws:
        filename = f"{law_name}_ì „ë¬¸.txt"
        try:
            save_law_text_by_name(law_name, filename)
        except Exception as e:
            print(f"âŒ {law_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
